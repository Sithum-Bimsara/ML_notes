{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9166a31e-2875-4f9d-be5e-4f45d98223c3",
   "metadata": {},
   "source": [
    "# üìò Naive Bayes Classifier - Explanation & Applications\n",
    "\n",
    "## üß† Question 1: Match the Naive Bayes Classifiers with Feature Types\n",
    "\n",
    "| **Naive Bayes Classifier** | **Feature Type** |\n",
    "|---------------------------|-----------------|\n",
    "| Gaussian Naive Bayes     | Continuous      |\n",
    "| Categorical Naive Bayes  | Categorical     |\n",
    "| Bernoulli Naive Bayes    | Binary          |\n",
    "\n",
    "### üìå Explanation:\n",
    "1. **Gaussian Naive Bayes** is used when the features are **continuous** and follows a normal distribution.\n",
    "2. **Categorical Naive Bayes** is best for **categorical** features.\n",
    "3. **Bernoulli Naive Bayes** is applied when the features are **binary** (0/1, Yes/No).\n",
    "\n",
    "---\n",
    "\n",
    "## ü§ñ Question 2: Applications of Naive Bayes Classifier\n",
    "\n",
    "‚úÖ **Correct Answers:**\n",
    "- **Spam filtering** üìß\n",
    "- **Recommendation systems** üéØ\n",
    "- **Sentiment analysis** üòäüò†\n",
    "- **Real-time predictions** ‚è≥\n",
    "\n",
    "### üìå Explanation:\n",
    "Naive Bayes is widely used in NLP (Natural Language Processing), classification tasks, and real-time predictions due to its simplicity and efficiency.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚ùì Question 3: Does Naive Bayes Perform Better with Numerical Inputs?\n",
    "\n",
    "‚ùå **False**\n",
    "\n",
    "### üìå Explanation:\n",
    "- Na√Øve Bayes classifiers are typically more effective with categorical variables rather than numerical ones. This is because they assume conditional independence between features given the class and often model categorical variables using probabilities directly (e.g., categorical distributions).\n",
    "\n",
    "- For numerical variables, Na√Øve Bayes often assumes a Gaussian (normal) distribution (in the case of Gaussian Na√Øve Bayes) or uses techniques like kernel density estimation. However, this assumption may not always hold in real-world datasets, potentially leading to suboptimal performance.\n",
    "\n",
    "- In contrast, categorical variables align well with the probability-based structure of Na√Øve Bayes, as they can be directly counted and used in likelihood estimation (e.g., using multinomial or Bernoulli Na√Øve Bayes).\n",
    "\n",
    "- Thus, while Na√Øve Bayes can handle both types of data, it often performs better with categorical variables.\n",
    "\n",
    " - The performance of a Naive Bayes classifier is not inherently better with numerical input variables compared to categorical variables. The effectiveness of Naive Bayes depends on the nature of the data and the assumptions made about the distribution of the input variables.\n",
    "\n",
    "- Categorical Variables: Naive Bayes works naturally with categorical variables, especially when using the Multinomial Naive Bayes or Bernoulli Naive Bayes variants. These models assume that the features are categorical and can handle them directly without any need for transformation.\n",
    "\n",
    "- Numerical Variables: When dealing with numerical variables, Naive Bayes typically assumes that the data follows a Gaussian (normal) distribution. This is known as Gaussian Naive Bayes. If the numerical data does not follow a Gaussian distribution, the performance of the classifier may degrade unless the data is transformed or a different distribution is assumed.\n",
    "\n",
    " - In summary, Naive Bayes can perform well with both categorical and numerical variables, but the key is to use the appropriate variant of the algorithm and ensure that the assumptions about the data distribution are met. Therefore, it is not accurate to say that Naive Bayes always performs better with numerical variables than with categorical ones.\n",
    "\n",
    "---\n",
    "\n",
    "## üè• Question 4: Sensitivity and Specificity in Covid-19 Diagnosis\n",
    "\n",
    "‚úÖ **Correct Answers:**\n",
    "- **The probability that an actual positive will test positive is 85%**\n",
    "- **The probability that an actual negative will test negative is 95%**\n",
    "\n",
    "### üìå Explanation:\n",
    "- **Sensitivity (True Positive Rate)**: Probability of correctly detecting a positive case (85%).\n",
    "- **Specificity (True Negative Rate)**: Probability of correctly detecting a negative case (95%).\n",
    "\n",
    "---\n",
    "\n",
    "## üìä Question 5: MAP Estimation in Naive Bayes\n",
    "\n",
    "‚úÖ **True**\n",
    "\n",
    "### üìå Explanation:\n",
    "MAP (Maximum A Posteriori) estimation simplifies Bayes' theorem by ignoring the denominator (evidence), making computations easier.\n",
    "\n",
    "# MAP Estimation in Bayesian Classifiers\n",
    "\n",
    "## Understanding MAP Estimation\n",
    "\n",
    "### Bayes' Theorem\n",
    "Bayes' Theorem is the fundamental formula used in Bayesian classification:\n",
    "\n",
    "\\[\n",
    "P(C_k | X) = \\frac{P(X | C_k) P(C_k)}{P(X)}\n",
    "\\]\n",
    "\n",
    "Where:\n",
    "- \\( P(C_k | X) \\) is the **posterior probability** (the probability of class \\( C_k \\) given data \\( X \\)).  \n",
    "- \\( P(X | C_k) \\) is the **likelihood** (the probability of data \\( X \\) given class \\( C_k \\)).  \n",
    "- \\( P(C_k) \\) is the **prior probability** of class \\( C_k \\).  \n",
    "- \\( P(X) \\) is the **evidence** (also called the normalizing constant).  \n",
    "\n",
    "### Maximum A Posteriori (MAP) Estimation\n",
    "MAP estimation finds the most probable class by maximizing the posterior probability:\n",
    "\n",
    "\\[\n",
    "\\hat{C}_{MAP} = \\arg\\max_{C_k} P(C_k | X)\n",
    "\\]\n",
    "\n",
    "Using Bayes' theorem:\n",
    "\n",
    "\\[\n",
    "\\hat{C}_{MAP} = \\arg\\max_{C_k} \\frac{P(X | C_k) P(C_k)}{P(X)}\n",
    "\\]\n",
    "\n",
    "Since **\\( P(X) \\) (evidence) is independent of class** and remains constant for all class comparisons, it **can be ignored**. Thus, MAP simplifies to:\n",
    "\n",
    "\\[\n",
    "\\hat{C}_{MAP} = \\arg\\max_{C_k} P(X | C_k) P(C_k)\n",
    "\\]\n",
    "\n",
    "### Why Ignore the Evidence Term?\n",
    "- \\( P(X) \\) is the same for all possible classes, so it **does not affect the ranking** of classes.\n",
    "- Eliminating \\( P(X) \\) **simplifies the optimization** by focusing only on the **likelihood** \\( P(X | C_k) \\) and **prior** \\( P(C_k) \\).\n",
    "\n",
    "### Key Takeaways\n",
    "- MAP estimation is used in **Bayesian classifiers**.\n",
    "- **It ignores the evidence term** \\( P(X) \\) because it is constant across all classes.\n",
    "- This **simplifies the optimization process**, making it computationally efficient.\n",
    "\n",
    "### Conclusion\n",
    "Since MAP estimation **ignores the evidence term and simplifies the optimization**, the statement that *\"MAP estimation is used in Bayes classifiers and it ignores the evidence term and simplifies the optimization.\"* is **True**.\n",
    "\n",
    "---\n",
    "\n",
    "## üßÆ Question 6: Precision in the Naive Bayes Equation\n",
    "\n",
    "‚úÖ **Correct Answer:**\n",
    "- **P(Y/X)**\n",
    "\n",
    "### üìå Explanation:\n",
    "Precision is defined as **P(Y/X)**, which represents the probability of the event given the evidence.\n",
    "\n",
    "---\n",
    "\n",
    "## üß† Question 7: Naive Bayes Assumptions\n",
    "\n",
    "‚úÖ **Correct Answer:**\n",
    "- **Both A and B**\n",
    "  - Assumes all features are **independent**\n",
    "  - Assumes all features are **equally important**\n",
    "\n",
    "### üìå Explanation:\n",
    "Naive Bayes assumes that all features contribute equally and are independent, which may not always hold in real-world data.\n",
    "\n",
    "---\n",
    "\n",
    "## üö® Question 8: Zero Frequency Problem\n",
    "\n",
    "‚úÖ **Correct Answers:**\n",
    "- **Laplace smoothing can be used to avoid Zero Frequency problem**\n",
    "- **An elbow plot or cross-validation can be used to determine the smoothing parameter**\n",
    "- **If an attribute value in the test set has no examples in the training set, the posterior probability will be zero**\n",
    "\n",
    "### üìå Explanation:\n",
    "Laplace smoothing helps prevent probabilities from becoming zero, which would otherwise lead to computational issues.\n",
    "\n",
    "# Zero Frequency Problem in Na√Øve Bayes\n",
    "\n",
    "## What is the Zero Frequency Problem?\n",
    "The Zero Frequency problem occurs in the Na√Øve Bayes algorithm when a categorical feature value appears in the test dataset but was never observed in the training dataset. This results in a probability of zero for the entire class, making classification impossible.\n",
    "\n",
    "### Mathematical Explanation\n",
    "The posterior probability is calculated as:\n",
    "\n",
    "\\[\n",
    "P(Y|X) = \\frac{P(X|Y) P(Y)}{P(X)}\n",
    "\\]\n",
    "\n",
    "If any feature \\(X\\) has a probability of zero due to missing data in the training set, then \\(P(Y|X)\\) becomes zero, causing an incorrect classification.\n",
    "\n",
    "---\n",
    "## Example: Zero Frequency Problem in Action\n",
    "\n",
    "### Scenario: Email Spam Classification\n",
    "Imagine we are building a Na√Øve Bayes classifier to detect spam emails. Our training dataset contains emails labeled as **Spam** \\((Y = Spam)\\) or **Not Spam** \\((Y = Not Spam)\\).\n",
    "\n",
    "The features are individual words appearing in the email. Suppose we have trained our classifier on a dataset where the word **\"Bitcoin\"** has never appeared in any email.\n",
    "\n",
    "### Na√Øve Bayes Formula Recap\n",
    "\\[\n",
    "P(Y|X) = \\frac{P(X|Y) P(Y)}{P(X)}\n",
    "\\]\n",
    "Where:\n",
    "- \\(P(Y|X)\\) = Posterior probability (the probability of the email being spam given the word \"Bitcoin\" appears).\n",
    "- \\(P(X|Y)\\) = Likelihood (the probability of the word \"Bitcoin\" appearing in a spam email).\n",
    "- \\(P(Y)\\) = Prior probability (overall probability of an email being spam).\n",
    "- \\(P(X)\\) = Evidence (the probability of the word \"Bitcoin\" appearing in any email).\n",
    "\n",
    "### How the Zero Frequency Problem Occurs\n",
    "\n",
    "| Word      | Count in Spam Emails | Count in Not Spam Emails |\n",
    "|-----------|---------------------|-------------------------|\n",
    "| Free      | 50                  | 10                      |\n",
    "| Money     | 40                  | 5                       |\n",
    "| Offer     | 30                  | 8                       |\n",
    "| Bitcoin   | 0                   | 0                       |\n",
    "\n",
    "Now, when a new email arrives containing the word **\"Bitcoin\"**, we compute:\n",
    "\n",
    "\\[\n",
    "P(Spam|Bitcoin) = \\frac{P(Bitcoin|Spam) P(Spam)}{P(Bitcoin)}\n",
    "\\]\n",
    "\n",
    "But in our training data, **Bitcoin never appeared before** \\( (P(Bitcoin|Spam) = 0) \\).\n",
    "\n",
    "Thus,\n",
    "\\[\n",
    "P(Spam|Bitcoin) = \\frac{0 \\times P(Spam)}{P(Bitcoin)} = 0\n",
    "\\]\n",
    "\n",
    "Since this probability is zero, Na√Øve Bayes **completely ignores all other words in the email** and incorrectly classifies it as **Not Spam**, even if it contains other spam-like words.\n",
    "\n",
    "---\n",
    "## Solution: Laplace Smoothing (Additive Smoothing)\n",
    "To avoid this problem, we use **Laplace Smoothing**, where we add a small value (e.g., 1) to every word count, ensuring that no probability becomes zero.\n",
    "\n",
    "### Formula with Laplace Smoothing\n",
    "\\[\n",
    "P(X_i | Y) = \\frac{count(X_i, Y) + \\alpha}{count(Y) + \\alpha \\times |V|}\n",
    "\\]\n",
    "Where:\n",
    "- \\( \\alpha \\) is the smoothing parameter (usually 1 for Laplace smoothing).\n",
    "- \\( |V| \\) is the number of unique feature values.\n",
    "\n",
    "### Why Laplace Smoothing Works\n",
    "- Instead of assigning **zero probability** to unseen words, it assigns a **small probability**.\n",
    "- This ensures that the Na√Øve Bayes classifier continues to function correctly.\n",
    "\n",
    "---\n",
    "## Understanding the Correct and Incorrect Choices\n",
    "\n",
    "### ‚úÖ Why (a) is Correct?\n",
    "‚úî Laplace Smoothing (Additive Smoothing) is a common technique used to handle the Zero Frequency problem. It prevents zero probabilities by adding a small constant (e.g., 1) to all counts.\n",
    "\n",
    "### ‚úÖ Why (b) is Correct?\n",
    "‚úî An **elbow plot** or **cross-validation** can be used to find the best smoothing parameter \\( \\alpha \\).\n",
    "- If \\( \\alpha \\) is too small, it won‚Äôt effectively handle zero probabilities.\n",
    "- If \\( \\alpha \\) is too large, it may distort the actual probabilities.\n",
    "- Cross-validation helps optimize this balance.\n",
    "\n",
    "### ‚úÖ Why (d) is Correct?\n",
    "‚úî If a feature value is missing from the training data, the likelihood \\(P(X|Y)\\) becomes zero, which results in the entire posterior probability being zero.\n",
    "\n",
    "### ‚ùå Why (c) is Incorrect?\n",
    "0 is **NOT** a good value for the smoothing parameter.\n",
    "- If \\( \\alpha = 0 \\), then there is **no smoothing applied**, and the Zero Frequency problem remains unsolved.\n",
    "\n",
    "---\n",
    "## Final Answer:\n",
    "‚úî **(a)** Laplace smoothing can be used to avoid the Zero Frequency problem.\n",
    "‚úî **(b)** An elbow plot or cross-validation can be used to determine the smoothing parameter.\n",
    "‚úî **(d)** In a Na√Øve Bayes algorithm, when an attribute value in the testing record has no example in the training set, then the entire posterior probability will be zero.\n",
    "\n",
    "---\n",
    "\n",
    "## üî¢ Question 9: Probability Calculation in Diagnostic Tests\n",
    "\n",
    "‚úÖ **Correct Answer:**\n",
    "- **0.029**\n",
    "\n",
    "### üìå Explanation:\n",
    "Using Bayes' Theorem, we calculate the probability that a person is a sufferer given a positive test result.\n",
    "\n",
    "---\n",
    "\n",
    "## üìÑ Question 10: Naive Bayes Characteristics\n",
    "\n",
    "‚úÖ **Correct Answers:**\n",
    "- **It is commonly used in text classification**\n",
    "- **It is based on Bayes theorem**\n",
    "- **It assumes conditional independence between input features**\n",
    "\n",
    "### üìå Explanation:\n",
    "Naive Bayes is widely used for text classification (e.g., spam detection) and relies on **Bayes' theorem** and the **assumption of feature independence**.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚ö†Ô∏è Question 11: Disadvantages of Naive Bayes\n",
    "\n",
    "‚úÖ **Correct Answers:**\n",
    "- **The independence assumption is not realistic in many real-world situations**\n",
    "- **Naive Bayes can‚Äôt accurately capture the interdependencies among features**\n",
    "- **The posterior probability in Naive Bayes might not be reliable**\n",
    "\n",
    "### üìå Explanation:\n",
    "While Naive Bayes is fast and effective, it assumes **feature independence**, which is often unrealistic, leading to reduced accuracy in complex datasets.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Question 12: Advantages of Naive Bayes\n",
    "\n",
    "‚úÖ **Correct Answers:**\n",
    "- **Can be used for Binary as well as Multi-class classification**\n",
    "- **Suitable for real-time classification tasks**\n",
    "- **Suitable for multi-class classification tasks**\n",
    "- **One of the fast and easy ML algorithms**\n",
    "\n",
    "### üìå Explanation:\n",
    "Naive Bayes is computationally efficient, making it a great choice for real-time applications and multi-class classification problems.\n",
    "\n",
    "---\n",
    "\n",
    "## üîç Question 13: Predicting Outcomes with Naive Bayes\n",
    "\n",
    "\n",
    "# Naive Bayes Classification: Predicting Play Outcome\n",
    "\n",
    "## Training Dataset:\n",
    "\n",
    "| Outlook  | Temperature | Humidity | Windy | Play |\n",
    "|----------|------------|----------|-------|------|\n",
    "| Sunny    | Hot        | Normal   | No    | Yes  |\n",
    "| Overcast | Mild       | Normal   | No    | Yes  |\n",
    "| Rainy    | Cool       | High     | Yes   | No   |\n",
    "| Sunny    | Mild       | Normal   | Yes   | No   |\n",
    "| Overcast | Hot        | High     | No    | No   |\n",
    "\n",
    "## Test Instance:\n",
    "\n",
    "| Outlook  | Temperature | Humidity | Windy | Play |\n",
    "|----------|------------|----------|-------|------|\n",
    "| Overcast | Hot        | Normal   | No    | ???  |\n",
    "\n",
    "## Step 1: Calculate Prior Probabilities\n",
    "\n",
    "Total instances: **5**\n",
    "\n",
    "\\[ P(Play = Yes) = \\frac{2}{5} = 0.4 \\]\n",
    "\n",
    "\\[ P(Play = No) = \\frac{3}{5} = 0.6 \\]\n",
    "\n",
    "---\n",
    "## Step 2: Calculate Likelihoods\n",
    "\n",
    "For each feature, calculate the conditional probabilities given the class.\n",
    "\n",
    "### Outlook = Overcast\n",
    "\\[ P(Outlook = Overcast | Play = Yes) = \\frac{1}{2} = 0.5 \\]\n",
    "\\[ P(Outlook = Overcast | Play = No) = \\frac{1}{3} \\approx 0.333 \\]\n",
    "\n",
    "### Temperature = Hot\n",
    "\\[ P(Temperature = Hot | Play = Yes) = \\frac{1}{2} = 0.5 \\]\n",
    "\\[ P(Temperature = Hot | Play = No) = \\frac{1}{3} \\approx 0.333 \\]\n",
    "\n",
    "### Humidity = Normal\n",
    "\\[ P(Humidity = Normal | Play = Yes) = \\frac{2}{2} = 1 \\]\n",
    "\\[ P(Humidity = Normal | Play = No) = \\frac{1}{3} \\approx 0.333 \\]\n",
    "\n",
    "### Windy = No\n",
    "\\[ P(Windy = No | Play = Yes) = \\frac{2}{2} = 1 \\]\n",
    "\\[ P(Windy = No | Play = No) = \\frac{1}{3} \\approx 0.333 \\]\n",
    "\n",
    "---\n",
    "## Step 3: Calculate Posterior Probabilities\n",
    "\n",
    "Using Naive Bayes:\n",
    "\n",
    "\\[\n",
    "P(Play = Yes | X) \\propto P(Play = Yes) \\times P(Outlook = Overcast | Play = Yes) \\times P(Temperature = Hot | Play = Yes) \\times P(Humidity = Normal | Play = Yes) \\times P(Windy = No | Play = Yes)\n",
    "\\]\n",
    "\n",
    "\\[\n",
    "P(Play = Yes | X) \\propto 0.4 \\times 0.5 \\times 0.5 \\times 1 \\times 1 = 0.1\n",
    "\\]\n",
    "\n",
    "\\[\n",
    "P(Play = No | X) \\propto P(Play = No) \\times P(Outlook = Overcast | Play = No) \\times P(Temperature = Hot | Play = No) \\times P(Humidity = Normal | Play = No) \\times P(Windy = No | Play = No)\n",
    "\\]\n",
    "\n",
    "\\[\n",
    "P(Play = No | X) \\propto 0.6 \\times 0.333 \\times 0.333 \\times 0.333 \\times 0.333 \\approx 0.0074\n",
    "\\]\n",
    "\n",
    "---\n",
    "## Step 4: Normalize Probabilities\n",
    "\n",
    "Total probability:\n",
    "\\[\n",
    "0.1 + 0.0074 \\approx 0.1074\n",
    "\\]\n",
    "\n",
    "\\[\n",
    "P(Play = Yes | X) = \\frac{0.1}{0.1074} \\approx 0.931\n",
    "\\]\n",
    "\n",
    "\\[\n",
    "P(Play = No | X) = \\frac{0.0074}{0.1074} \\approx 0.069\n",
    "\\]\n",
    "\n",
    "---\n",
    "## Step 5: Prediction\n",
    "\n",
    "Since \\( P(Play = Yes | X) > P(Play = No | X) \\), the predicted outcome is **\"Yes\"**.\n",
    "\n",
    "### Final Answer:\n",
    "The predicted \"Play\" outcome for the given day is **\"Yes\"**.\n",
    "\n",
    "\n",
    "‚úÖ **Correct Answer:**\n",
    "- **Yes**\n",
    "\n",
    "### üìå Explanation:\n",
    "Using the Naive Bayes formula, we can predict outcomes based on prior probabilities and likelihood estimates.\n",
    "\n",
    "---\n",
    "\n",
    "## üìà Question 14: Sensitivity, Specificity, and Precision of RAT (Covid-19 Test)\n",
    "\n",
    "\n",
    "# Analysis of Rapid Antigen Test (RAT) for Covid-19\n",
    "\n",
    "## Given Data\n",
    "\n",
    "### Confusion Matrix:\n",
    "|                | Covid-19 Positive | Covid-19 Negative | Total  |\n",
    "|---------------|------------------|------------------|--------|\n",
    "| **RAT Positive** | 378              | 397              | 775    |\n",
    "| **RAT Negative** | 2                | 98,823           | 98,825 |\n",
    "| **Total**       | 390              | 99,220           | 100,000 |\n",
    "\n",
    "### Definitions:\n",
    "\n",
    "#### **Sensitivity (True Positive Rate)**:\n",
    "\\[\n",
    "Sensitivity = \\frac{TP}{TP + FN}\n",
    "\\]\n",
    "\n",
    "#### **Specificity (True Negative Rate)**:\n",
    "\\[\n",
    "Specificity = \\frac{TN}{TN + FP}\n",
    "\\]\n",
    "\n",
    "#### **Precision**:\n",
    "\\[\n",
    "Precision = \\frac{TP}{TP + FP}\n",
    "\\]\n",
    "\n",
    "## Step 1: Identify Values from the Confusion Matrix\n",
    "- **True Positives (TP)** = 378 (RAT Positive and Covid-19 Positive)\n",
    "- **False Positives (FP)** = 397 (RAT Positive and Covid-19 Negative)\n",
    "- **True Negatives (TN)** = 98,823 (RAT Negative and Covid-19 Negative)\n",
    "- **False Negatives (FN)** = 2 (RAT Negative and Covid-19 Positive)\n",
    "\n",
    "## Step 2: Calculate Sensitivity\n",
    "\\[\n",
    "Sensitivity = \\frac{TP}{TP + FN} = \\frac{378}{378 + 2} = \\frac{378}{380} \\approx 0.995\n",
    "\\]\n",
    "\n",
    "## Step 3: Calculate Specificity\n",
    "\\[\n",
    "Specificity = \\frac{TN}{TN + FP} = \\frac{98,823}{98,823 + 397} = \\frac{98,823}{99,220} \\approx 0.996\n",
    "\\]\n",
    "\n",
    "## Step 4: Calculate Precision\n",
    "\\[\n",
    "Precision = \\frac{TP}{TP + FP} = \\frac{378}{378 + 397} = \\frac{378}{775} \\approx 0.488\n",
    "\\]\n",
    "\n",
    "## Step 5: Match with Given Options\n",
    "- **Sensitivity ‚âà 0.995**\n",
    "- **Specificity ‚âà 0.996**\n",
    "- **Precision ‚âà 0.488**\n",
    "\n",
    "### **Final Answer:**\n",
    "The correct statement is:\n",
    "\n",
    "> **b. Sensitivity = 0.995, Specificity = 0.996, Precision = 0.488**\n",
    ">\n",
    "> \n",
    "‚úÖ **Correct Answer:**\n",
    "- **Sensitivity = 0.995, Specificity = 0.996, Precision = 0.488**\n",
    "\n",
    "### üìå Explanation:\n",
    "- **Sensitivity**: Ability to detect true positives.\n",
    "- **Specificity**: Ability to detect true negatives.\n",
    "- **Precision**: Percentage of true positives among all positive results.\n",
    "\n",
    "---\n",
    "\n",
    "## üî¨ Question 15: Matching Bayes Equation Components\n",
    "\n",
    "‚úÖ **Correct Matches:**\n",
    "\n",
    "| **Bayes Equation Component** | **Meaning** |\n",
    "|---------------------------|-----------------|\n",
    "| **P(A|B)**  | Posterior probability üìå |\n",
    "| **P(B|A)**  | Likelihood üìä |\n",
    "| **P(A)**    | Prior probability üî¢ |\n",
    "| **P(B)**    | Evidence üß© |\n",
    "\n",
    "### üìå Explanation:\n",
    "- **Posterior Probability (P(A|B))**: Probability of A occurring given B.\n",
    "- **Likelihood (P(B|A))**: Probability of B occurring given A.\n",
    "- **Prior Probability (P(A))**: Initial probability of A.\n",
    "- **Evidence (P(B))**: Total probability of B occurring.\n",
    "\n",
    "---\n",
    "\n",
    "üéâ **Conclusion:**\n",
    "Naive Bayes is a powerful classification algorithm with various applications, but it has limitations due to the independence assumption. It works well for text classification, spam detection, and medical diagnoses. üöÄ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b059da-e277-43d8-97ac-fda21ab2b670",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
